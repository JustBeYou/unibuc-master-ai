{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dd85952",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, sent_tokenize, pos_tag\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import stopwords\n",
    "from pprint import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e021368",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8196671e",
   "metadata": {},
   "source": [
    "## Original Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b832bd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://dl.acm.org/doi/pdf/10.1145/318723.318728\n",
    "def original_lesk(context_tokens, ambiguous_word, pos=None):    \n",
    "    word_synsets = wordnet.synsets(ambiguous_word)\n",
    "    \n",
    "    # Automatically detect the POS of the first occurence of the ambiguous word\n",
    "    if pos == 'auto':\n",
    "        context_tags = pos_tag(context_tokens)\n",
    "        for word, tag in context_tags:\n",
    "            if word != ambiguous_word:\n",
    "                continue\n",
    "                \n",
    "            tag = penn2morphy(tag)\n",
    "            if tag is None:\n",
    "                continue\n",
    "                \n",
    "            pos = tag\n",
    "            break\n",
    "    \n",
    "    if pos is not None:\n",
    "        word_synsets = list(filter(lambda synset: synset.pos() == pos, word_synsets))\n",
    "        \n",
    "    \n",
    "    # Gather all words from all definitions of the surrounding words\n",
    "    context = set()\n",
    "    for context_token in set(context_tokens):\n",
    "        if context_token == ambiguous_word:\n",
    "            continue\n",
    "        \n",
    "        synsets = wordnet.synsets(context_token)\n",
    "        \n",
    "        for synset in synsets:\n",
    "            for token in synset.definition().replace(\";\", \" \").split():\n",
    "                context.add(token)\n",
    "    \n",
    "    # Remove the stopwords\n",
    "    context = context.difference(english_stopwords)\n",
    "    \n",
    "    _, best_sense = max([(lesk_measure(context, sense), sense) for sense in word_synsets])\n",
    "        \n",
    "    return best_sense\n",
    "    \n",
    "def lesk_measure(context: set, sense):\n",
    "    definition_tokens = set(sense.definition().replace(\";\", \" \").split())\n",
    "    return len(context.intersection(definition_tokens))\n",
    "\n",
    "# https://stackoverflow.com/questions/35458896/python-map-nltk-stanford-pos-tags-to-wordnet-pos-tags\n",
    "morphy_tag = {\n",
    "    'NN':wordnet.NOUN, \n",
    "    'JJ':wordnet.ADJ,\n",
    "    'VB':wordnet.VERB, \n",
    "    'RB':wordnet.ADV\n",
    "}\n",
    "\n",
    "def penn2morphy(penntag):\n",
    "    penntag = penntag[:2]\n",
    "    return morphy_tag[penntag] if penntag in morphy_tag else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d4576c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = word_tokenize('Students enjoy going to school, studying and reading books.')\n",
    "word = 'school'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bcf3187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original lesk: Synset('school.n.01') an educational institution\n"
     ]
    }
   ],
   "source": [
    "syn = original_lesk(text, word, 'auto')\n",
    "print(\"Original lesk:\", syn, syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20822e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK lesk: Synset('school.n.06') an educational institution's faculty and students\n"
     ]
    }
   ],
   "source": [
    "syn = lesk(text, word, 'n')\n",
    "print(\"NLTK lesk:\", syn, syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55199c8e",
   "metadata": {},
   "source": [
    "## Extended Lesk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f69f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html\n",
    "default_extensions = [\n",
    "    'hypernyms',\n",
    "    'instance_hypernyms',\n",
    "    'hyponyms',\n",
    "    'instance_hyponyms',\n",
    "    'member_holonyms',\n",
    "    'substance_holonyms',\n",
    "    'part_holonyms',\n",
    "    'member_meronyms',\n",
    "    'substance_meronyms',\n",
    "    'part_meronyms',\n",
    "    'attributes',\n",
    "    'similar_tos',\n",
    "    'also_sees'\n",
    "]\n",
    "\n",
    "# https://www.researchgate.net/profile/Ted-Pedersen/publication/2563220_Extended_Gloss_Overlaps_as_a_Measure_of_Semantic_Relatedness/links/00b49520cec00a4b51000000/Extended-Gloss-Overlaps-as-a-Measure-of-Semantic-Relatedness.pdf?origin=publication_detail\n",
    "def extended_lesk(context_tokens, ambiguous_word, extensions = default_extensions):\n",
    "    ambiguous_word = ambiguous_word.lower()\n",
    "    word_synsets = wordnet.synsets(ambiguous_word)\n",
    "    \n",
    "    # Filter stopwords\n",
    "    context_tokens = filter(lambda token: token not in english_stopwords, context_tokens)\n",
    "    context_tokens = list(context_tokens)\n",
    "    \n",
    "    # Collect words and their tags\n",
    "    context_tags = pos_tag(context_tokens)\n",
    "    context_tags = map(lambda wt: (wt[0].lower(), penn2morphy(wt[1])), context_tags)\n",
    "    context_tags = list(filter(lambda wt: wt[1] is not None, context_tags))\n",
    "    \n",
    "    context_synsets = []\n",
    "    target_word_synsets = []\n",
    "    \n",
    "    # Collect the extended gloss for each word\n",
    "    for word, tag in context_tags:\n",
    "        syns = wordnet.synsets(word)\n",
    "        syns = list(filter(lambda s: s.pos() == tag, syns))\n",
    "        \n",
    "        extended_syns = [getattr(syn, ext)() for ext in extensions for syn in syns]\n",
    "        extended_syns = [syn for syn_list in extended_syns for syn in syn_list]\n",
    "        \n",
    "        if word == ambiguous_word:\n",
    "            target_word_synsets.extend(syns)\n",
    "        else:\n",
    "            context_synsets.extend(syns)\n",
    "            context_synsets.extend(extended_syns)\n",
    "    \n",
    "    # Tokenize and filter stopwords\n",
    "    _, best_sense = max([(extended_lesk_measure_context(context_synsets, sense), sense) for sense in target_word_synsets])\n",
    "    \n",
    "    return best_sense\n",
    "\n",
    "def sense_to_text(sense):\n",
    "    text = sense.definition().replace(\";\", \" \").split()\n",
    "    return list(filter(lambda word: word not in english_stopwords, text))\n",
    "\n",
    "def extended_lesk_measure_context(context, target_sense):\n",
    "    target_sense = sense_to_text(target_sense)\n",
    "    overlap = 0\n",
    "    for context_sense in context:\n",
    "        overlap += extended_lesk_measure(sense_to_text(context_sense), target_sense)\n",
    "    return overlap\n",
    "\n",
    "def extended_lesk_measure(a, b):\n",
    "    a, b = a.copy(), b.copy()\n",
    "    overlap = 0\n",
    "    \n",
    "    # Find all overlapping phrases first\n",
    "    while True:\n",
    "        max_len, (a_pos, b_pos) = longest_common_subarray(a, b)\n",
    "\n",
    "        if max_len < 2:\n",
    "            break\n",
    "            \n",
    "        overlap += max_len ** 2\n",
    "        \n",
    "        a[a_pos:a_pos+max_len] = '#'\n",
    "        b[b_pos:b_pos+max_len] = '$'\n",
    "    \n",
    "    # When no overlapping phrases are left, do an element-wise intersection\n",
    "    return overlap + len(set(a).intersection(set(b)))\n",
    "\n",
    "def longest_common_subarray(a, b):\n",
    "    n, m = len(a), len(b)\n",
    "    dp = [[0 for _ in range(m + 1)] for _ in range(n + 1)]    \n",
    "    max_len, max_pos = 0, (-1, -1)\n",
    "    \n",
    "    for i in range(n - 1, -1, -1):\n",
    "        for j in range(m - 1, -1, -1):\n",
    "            if a[i] == b[j]:\n",
    "                dp[i][j] = dp[i + 1][j + 1] + 1\n",
    "                if dp[i][j] > max_len:\n",
    "                    max_len = dp[i][j]\n",
    "                    max_pos = (i, j)\n",
    "                    \n",
    "    return max_len, max_pos\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb3b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCS example (3, (3, 2))\n"
     ]
    }
   ],
   "source": [
    "print(\"LCS example\", longest_common_subarray(\n",
    "    [1, 2, 3, 4, 5, 6, 7, 9],\n",
    "    [1, 2, 4, 5, 6, 9, 10, 11, 12]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf294292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended lesk measure example:  14\n"
     ]
    }
   ],
   "source": [
    "print(\"Extended lesk measure example: \",extended_lesk_measure(\n",
    "    [1, 2, 3, 4, 5, 6, 7, 9],\n",
    "    [1, 2, 4, 5, 6, 9, 10, 11, 12]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ff5ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Lesk 1: Synset('school.n.05') the period of instruction in a school; the time period when school is in session\n"
     ]
    }
   ],
   "source": [
    "syn = extended_lesk(text, word)\n",
    "print(\"Extended Lesk 1:\", syn, syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3cb3ba9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Lesk 2: Synset('school.n.01') an educational institution\n"
     ]
    }
   ],
   "source": [
    "syn = extended_lesk(text, word, extensions = [\n",
    "    \n",
    "])\n",
    "print(\"Extended Lesk 2:\", syn, syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7956a577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Lesk 3: Synset('school.n.01') an educational institution\n"
     ]
    }
   ],
   "source": [
    "syn = extended_lesk(text, word, extensions = [\n",
    "    'member_meronyms',\n",
    "    'substance_meronyms',\n",
    "    'part_meronyms',\n",
    "    'attributes',\n",
    "    'similar_tos',\n",
    "    'also_sees'\n",
    "])\n",
    "print(\"Extended Lesk 3:\", syn, syn.definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c167260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended Lesk 4: Synset('school.n.05') the period of instruction in a school; the time period when school is in session\n"
     ]
    }
   ],
   "source": [
    "syn = extended_lesk(text, word, extensions = [\n",
    "    'hypernyms',\n",
    "    'instance_hypernyms',\n",
    "    'hyponyms',\n",
    "    'instance_hyponyms',\n",
    "    'member_holonyms',\n",
    "])\n",
    "print(\"Extended Lesk 4:\", syn, syn.definition())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533a531",
   "metadata": {},
   "source": [
    "## Ant colony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "459384c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9851e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness(C, dictionary, cache, extensions):\n",
    "    s = 0\n",
    "    for a in C:\n",
    "        for b in C:\n",
    "            s += extended_lesk_measure(a, b, dictionary, cache, extensions)\n",
    "    return s\n",
    "\n",
    "def extended_lesk_measure(a, b, dictionary, cache, extensions):\n",
    "    # Support for both word vectors and synset names\n",
    "    word_vectors = type(a) is list and type(b) is list\n",
    "    if not word_vectors:\n",
    "        key_1 = (a.name(), b.name())\n",
    "        key_2 = (b.name(), a.name())\n",
    "    \n",
    "        if key_1 in cache: return cache[key_1]\n",
    "        if key_2 in cache: return cache[key_2]\n",
    "    \n",
    "        a = sense_to_tokens_set(a, dictionary, cache, extensions)['extended']\n",
    "        b = sense_to_tokens_set(b, dictionary, cache, extensions)['extended']\n",
    "    \n",
    "    # Intersection in O(max(N, M)) for two sorted arrays\n",
    "    overlap, i, j = 0, 0, 0\n",
    "    while i < len(a) and j < len(b) and a[i] is not None and b[j] is not None:\n",
    "        if a[i] == b[j]:\n",
    "            overlap += 1\n",
    "            i += 1\n",
    "            j += 1\n",
    "        elif a[i] < b[j]:\n",
    "            i += 1\n",
    "        else:\n",
    "            j += 1\n",
    "    \n",
    "    if not word_vectors:\n",
    "        cache[key_1] = overlap\n",
    "    \n",
    "    return overlap\n",
    "\n",
    "def sense_to_tokens_set(sense, dictionary, cache, extensions):\n",
    "    if sense.name() in cache:\n",
    "        return cache[sense.name()]\n",
    "    \n",
    "    # Get all Wordnet relations\n",
    "    extended_senses = [getattr(sense, ext)() for ext in extensions]\n",
    "    extended_senses = [current_sense for sense_list in extended_senses for current_sense in sense_list] + [sense]\n",
    "    \n",
    "    # Create a bag-of-words for all definitions\n",
    "    tokens = set()\n",
    "    self_tokens = None\n",
    "    \n",
    "    for current_sense in extended_senses:\n",
    "        current_tokens = set(\n",
    "            filter(\n",
    "                lambda word: word.isalpha() and word not in english_stopwords, \n",
    "                word_tokenize(current_sense.definition())\n",
    "            )\n",
    "        )\n",
    "        current_tokens = [word.lower() for word in current_tokens]\n",
    "        tokens.update(current_tokens)\n",
    "        \n",
    "        if sense.name() == current_sense.name():\n",
    "            self_tokens = set(current_tokens)\n",
    "    \n",
    "    # Obtain unique ids for each word and create a sorted array of word ids\n",
    "    token_ids = [-1 for _ in range(len(tokens))]\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in dictionary:\n",
    "            dictionary[token] = len(dictionary)\n",
    "            \n",
    "        token_ids[i] = dictionary[token]\n",
    "        \n",
    "    self_token_ids = [-1 for _ in range(len(self_tokens))]\n",
    "    for i, token in enumerate(self_tokens):\n",
    "        self_token_ids[i] = dictionary[token]\n",
    "        \n",
    "    token_ids.sort()\n",
    "    self_token_ids.sort()\n",
    "    \n",
    "    cache[sense.name()] = {'extended': token_ids, 'simple': self_token_ids}\n",
    "    return cache[sense.name()]\n",
    "\n",
    "default_extensions = [\n",
    "    'hypernyms',\n",
    "    'instance_hypernyms',\n",
    "    'hyponyms',\n",
    "    'instance_hyponyms',\n",
    "    'member_holonyms',\n",
    "    'substance_holonyms',\n",
    "    'part_holonyms',\n",
    "    'member_meronyms',\n",
    "    'substance_meronyms',\n",
    "    'part_meronyms',\n",
    "    'attributes',\n",
    "    'similar_tos',\n",
    "    'also_sees'\n",
    "]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a886676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " {'amount': 0,\n",
       "  'instruction': 1,\n",
       "  'session': 2,\n",
       "  'period': 3,\n",
       "  'time': 4,\n",
       "  'study': 5,\n",
       "  'aside': 6,\n",
       "  'day': 7,\n",
       "  'set': 8,\n",
       "  'school': 9,\n",
       "  'acquiring': 10,\n",
       "  'knowledge': 11,\n",
       "  'formally': 12,\n",
       "  'process': 13,\n",
       "  'educated': 14,\n",
       "  'gradual': 15},\n",
       " {'school.n.05': {'extended': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "   'simple': [1, 2, 3, 4, 9]},\n",
       "  'school.n.03': {'extended': [9, 10, 11, 12, 13, 14, 15],\n",
       "   'simple': [9, 12, 13, 14]},\n",
       "  ('school.n.05', 'school.n.03'): 1})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary = {}\n",
    "cache = {}\n",
    "\n",
    "result = extended_lesk_measure(\n",
    "    wordnet.synset('school.n.05'),\n",
    "    wordnet.synset('school.n.03'),\n",
    "    dictionary, cache, default_extensions\n",
    ")\n",
    "\n",
    "result, dictionary, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36d32819",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " {'amount': 0,\n",
       "  'instruction': 1,\n",
       "  'session': 2,\n",
       "  'period': 3,\n",
       "  'time': 4,\n",
       "  'study': 5,\n",
       "  'aside': 6,\n",
       "  'day': 7,\n",
       "  'set': 8,\n",
       "  'school': 9,\n",
       "  'acquiring': 10,\n",
       "  'knowledge': 11,\n",
       "  'formally': 12,\n",
       "  'process': 13,\n",
       "  'educated': 14,\n",
       "  'gradual': 15},\n",
       " {'school.n.05': {'extended': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "   'simple': [1, 2, 3, 4, 9]},\n",
       "  'school.n.03': {'extended': [9, 10, 11, 12, 13, 14, 15],\n",
       "   'simple': [9, 12, 13, 14]},\n",
       "  ('school.n.05', 'school.n.03'): 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = extended_lesk_measure(\n",
    "    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "    [1, 10, 11, 12, 13, 14, 15],\n",
    "    dictionary, cache, default_extensions\n",
    ")\n",
    "\n",
    "result, dictionary, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d87afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_parameters = {\n",
    "    'E_a': 5,        # energy taken by an ant\n",
    "    'E_max': 30,      # max energy of ant\n",
    "    'delta': 0.3,     # evaporation rate of cycles\n",
    "    'E_0': 10,        # initial enery in nodes\n",
    "    'omega': 5,      # ant life-span\n",
    "    'L_v': 100,        # odour vector length\n",
    "    'delta_v': 0.5,   # percent of odour deposited at a node\n",
    "    'c_ac': 20,        # number of cycles\n",
    "    'theta': 0.05,       # amount of ant pheromone\n",
    "}\n",
    "\n",
    "def ant_colony(text, extensions, parameters):\n",
    "    dictionary = {}\n",
    "    cache = {}\n",
    "    \n",
    "    # Pre-process and tokenize the input text\n",
    "    text = text.replace('\\n', ' ')\n",
    "    sentences = sent_tokenize(text)\n",
    "    sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "    sentences = [[word.lower() for word in sentence if word.isalpha()] for sentence in sentences]\n",
    "    sentences = [[(word, wordnet.synsets(word)) for word in sentence] for sentence in sentences]\n",
    "    sentences = [[(word, senses) for word, senses in sentence if len(senses) > 0] for sentence in sentences]\n",
    "    \n",
    "    graph, graph_edges = build_graph(sentences, dictionary, cache, extensions, parameters['E_0'], parameters['L_v'])\n",
    "    \n",
    "    nest_ids = [idx for idx, node in enumerate(graph) if node['type'] == 'sense']\n",
    "    nest_ids.sort()\n",
    "    \n",
    "    word_ids = [idx for idx, node in enumerate(graph) if node['type'] == 'word']\n",
    "    word_ids.sort()\n",
    "    best_C = None\n",
    "    C = [None for _ in range(len(word_ids))]\n",
    "    best_score = 0\n",
    "    \n",
    "    \n",
    "    next_ant_id = 0\n",
    "    ants = {}\n",
    "    \n",
    "    for cycle in range(parameters['c_ac']):\n",
    "        print(\"=== === ===\")\n",
    "        print(f\"Cycle {cycle+1}/{parameters['c_ac']}\")\n",
    "        \n",
    "        # Age all ants\n",
    "        for ant_id in ants:\n",
    "            ants[ant_id]['age'] += 1\n",
    "        \n",
    "        # (1) Eliminate dead ants\n",
    "        to_eliminate = [(ant_id, ant) for ant_id, ant in ants.items() if ant['age'] >= parameters['omega']]\n",
    "        for ant_id, ant in to_eliminate:\n",
    "            graph[ant['position']]['energy'] += ant['energy']\n",
    "            \n",
    "#             print(f\"Ant {ant_id} of {ant['motherland']} died at {ant['position']} - {str_node(graph[ant['position']])}\")\n",
    "        \n",
    "            del ants[ant_id]\n",
    "        print(f\"Dead ants {len(to_eliminate)}\")\n",
    "            \n",
    "        # (2) Generate new ants\n",
    "        new_ants = 0\n",
    "        for nest_id in nest_ids:\n",
    "            nest = graph[nest_id]\n",
    "            \n",
    "            if nest['energy'] == 0:\n",
    "                continue\n",
    "            \n",
    "            energy = nest['energy']\n",
    "            prob = np.arctan(energy) / np.pi + 0.5\n",
    "            should_generate = np.random.binomial(1, prob)\n",
    "            \n",
    "            if not should_generate:\n",
    "                continue\n",
    "               \n",
    "            new_ants += 1\n",
    "            ants[next_ant_id] = {\n",
    "                'motherland': nest['sense'].name(),\n",
    "                'energy': 1,\n",
    "                'age': 0,\n",
    "                'position': nest_id,\n",
    "                'odour': nest['odour'], # copy by reference because this will never be modified\n",
    "            }\n",
    "\n",
    "            graph[nest_id]['energy'] -= 1\n",
    "            \n",
    "#             print(f\"Ant {next_ant_id} was born: {ants[next_ant_id]}\")\n",
    "#             print(f\"New energy of the nest: {graph[nest_id]['energy']}\")\n",
    "            \n",
    "            next_ant_id += 1\n",
    "        print(f\"New ants {new_ants}\")\n",
    "    \n",
    "        # Pre-compute lesk measure for all nodes and ants\n",
    "        lesk_n_f = []\n",
    "        \n",
    "        for node in graph:\n",
    "            row = {}\n",
    "            \n",
    "            for ant_id, ant in ants.items():\n",
    "                lesk_score = extended_lesk_measure(node['odour'], ant['odour'], dictionary, cache, extensions)\n",
    "                row[ant_id] = lesk_score\n",
    "            \n",
    "            lesk_n_f.append(row)\n",
    "    \n",
    "        # Pre-compute stuff for seeking ant movement\n",
    "        eval_f_n_a = {}\n",
    "        \n",
    "        for node in graph:\n",
    "            total_eval_f_n_a = 0\n",
    "            \n",
    "            total_nodes_energy = 0\n",
    "            for edge in node['edges']:\n",
    "                next_node = graph[edge['node']]\n",
    "                total_nodes_energy += next_node['energy']\n",
    "            \n",
    "            for edge in node['edges']:\n",
    "                next_node = graph[edge['node']]\n",
    "                \n",
    "                eval_f_n = next_node['energy'] / total_nodes_energy\n",
    "                eval_f_a = 1 - graph_edges[edge['idx']]['pheromone']\n",
    "                \n",
    "                value = eval_f_n + eval_f_a\n",
    "                total_eval_f_n_a += value\n",
    "                \n",
    "                if next_node['idx'] not in eval_f_n_a:\n",
    "                    eval_f_n_a[next_node['idx']] = {}\n",
    "                    \n",
    "                eval_f_n_a[next_node['idx']][edge['idx']] = value\n",
    "                \n",
    "            if total_eval_f_n_a == 0:\n",
    "                for edge in node['edges']:\n",
    "                    next_node = graph[edge['node']]\n",
    "\n",
    "                    if next_node['idx'] not in eval_f_n_a:\n",
    "                        eval_f_n_a[next_node['idx']] = {}\n",
    "\n",
    "                    eval_f_n_a[next_node['idx']][edge['idx']] = 1/len(node['edges'])\n",
    "\n",
    "                continue\n",
    "                \n",
    "            for edge in node['edges']:\n",
    "                next_node = graph[edge['node']]\n",
    "                \n",
    "                eval_f_n_a[next_node['idx']][edge['idx']] /= total_eval_f_n_a\n",
    "            \n",
    "        # Pre-compute stuff for returning ant movement\n",
    "        return_eval_f_n_a = {}\n",
    "        \n",
    "        for node in graph:\n",
    "            return_total_eval_f_n_a = 0\n",
    "            \n",
    "            total_lesk = 0\n",
    "            for edge in node['edges']:\n",
    "                next_node = graph[edge['node']]\n",
    "                \n",
    "                total_lesk += lesk_n_f[next_node['idx']][ant_id]\n",
    "            \n",
    "            if total_lesk == 0:\n",
    "                for edge in node['edges']:\n",
    "                    next_node = graph[edge['node']]\n",
    "                \n",
    "                    if next_node['idx'] not in return_eval_f_n_a:\n",
    "                        return_eval_f_n_a[next_node['idx']] = {}\n",
    "                \n",
    "                    return_eval_f_n_a[next_node['idx']][edge['idx']] = 1/len(node['edges'])\n",
    "                \n",
    "                continue\n",
    "            \n",
    "            for edge in node['edges']:\n",
    "                next_node = graph[edge['node']]\n",
    "                \n",
    "                return_eval_f_n = lesk_n_f[next_node['idx']][ant_id] / total_lesk\n",
    "                return_eval_f_a = graph_edges[edge['idx']]['pheromone']\n",
    "                \n",
    "                value = return_eval_f_n + return_eval_f_a\n",
    "                return_total_eval_f_n_a += value\n",
    "                \n",
    "                if next_node['idx'] not in return_eval_f_n_a:\n",
    "                    return_eval_f_n_a[next_node['idx']] = {}\n",
    "                    \n",
    "                return_eval_f_n_a[next_node['idx']][edge['idx']] = value\n",
    "               \n",
    "            for edge in node['edges']:\n",
    "                next_node = graph[edge['node']]\n",
    "                \n",
    "                return_eval_f_n_a[next_node['idx']][edge['idx']] /= return_total_eval_f_n_a\n",
    "        \n",
    "    \n",
    "        # (3) Determine ant mode; move; create bridges\n",
    "        moves = []\n",
    "        \n",
    "        for ant_id, ant in ants.items():\n",
    "            return_mode_prob = ant['energy'] / parameters['E_max']\n",
    "            return_mode = np.random.binomial(1, return_mode_prob)\n",
    "            \n",
    "            ant_node = graph[ant['position']]\n",
    "            \n",
    "            if return_mode:\n",
    "                probs = [return_eval_f_n_a[edge['node']][edge['idx']] for edge in ant_node['edges']]\n",
    "            else:\n",
    "                probs = [eval_f_n_a[edge['node']][edge['idx']] for edge in ant_node['edges']]\n",
    "\n",
    "             \n",
    "            move_to = np.random.choice(ant_node['edges'], p=probs)\n",
    "            moves.append((ant_id, move_to))\n",
    "            \n",
    "    \n",
    "        # (4) Update the environment\n",
    "        for ant_id, move_to in moves:\n",
    "            next_node = graph[move_to['node']]\n",
    "            \n",
    "            graph_edges[move_to['idx']]['pheromone'] += parameters['theta']\n",
    "            graph_edges[move_to['idx']]['pheromone'] = min(graph_edges[move_to['idx']]['pheromone'], 1)\n",
    "            ants[ant_id]['position'] = next_node['idx']\n",
    "            \n",
    "            # Update the energy if we reached home or odours if we reached a regular node\n",
    "            if next_node['type'] == 'sense' and next_node['sense'].name() == ants[ant_id]['motherland']:\n",
    "                graph[move_to['node']]['energy'] += ants[ant_id]['energy'] - 1\n",
    "                ants[ant_id]['energy'] = 1\n",
    "            elif next_node['type'] != 'sense':\n",
    "                # Try to add the ant's odour to the node\n",
    "                for i, word in enumerate(ants[ant_id]['odour']):\n",
    "                    if len(next_node['odour']) < parameters['L_v']:\n",
    "                        graph[move_to['node']]['odour'].append(word)\n",
    "                    else:\n",
    "                        break\n",
    "                \n",
    "                # Replace random positons if the node is full\n",
    "                for j in range(i, len(ants[ant_id]['odour'])):\n",
    "                    rnd_idx = np.random.randint(0, len(graph[move_to['node']]['odour']))\n",
    "                    graph[move_to['node']]['odour'][rnd_idx] = ants[ant_id]['odour'][j]\n",
    "    \n",
    "        for i in range(len(graph_edges)):\n",
    "            graph_edges[i]['pheromone'] *= (1-parameters['delta'])\n",
    "    \n",
    "        # Compute the score of the generation\n",
    "#         word_ids = [idx for idx, node in enumerate(graph) if node['type'] == 'word']\n",
    "#         word_ids.sort()\n",
    "#         C = [graph[graph[word_id]['edges'][1]['node']]['sense'] for word_id in word_ids]\n",
    "    \n",
    "        for i, word_id in enumerate(word_ids):\n",
    "            best_energy = 0\n",
    "            best_sense = graph[graph[word_id]['edges'][1]['node']]['sense']\n",
    "            \n",
    "            for edge in graph[word_id]['edges'][1:]:\n",
    "                assert graph[edge['node']]['type'] == 'sense'\n",
    "                \n",
    "                if graph[edge['node']]['energy'] > best_energy:\n",
    "                    best_energy = graph[edge['node']]['energy']\n",
    "                    best_sense = graph[edge['node']]['sense']\n",
    "                \n",
    "            C[i] = best_sense\n",
    "    \n",
    "        score = fitness(C, dictionary, cache, extensions)\n",
    "        print(f\"Fitness score: {score}\")\n",
    "        \n",
    "        if score > best_score:\n",
    "            best_C = C.copy()\n",
    "            best_score = score\n",
    "    \n",
    "        print(\"=== === ===\")\n",
    "        print()\n",
    "        \n",
    "    print(f\"Solution (score = {best_score}):\")\n",
    "    for word_id, sense in zip(word_ids, best_C):\n",
    "        print(\"#\", graph[word_id]['word'], f\"f{sense.name()} =\", sense.definition())\n",
    "        print()\n",
    "    \n",
    "def str_node(node):\n",
    "    t = node['type']\n",
    "    s = t\n",
    "    \n",
    "    if t == 'word':\n",
    "        s += f'- {node[\"word\"]}'\n",
    "    elif t == 'sense':\n",
    "        s += f'- {node[\"sense\"].name()}'\n",
    "        \n",
    "    return s\n",
    "    \n",
    "def build_graph(sentences, dictionary, cache, extensions, initial_energy, odour_length):\n",
    "    graph = [{\n",
    "        'idx': 0, \n",
    "        'type': 'text', \n",
    "        'parent': None, \n",
    "        'edges': [],\n",
    "        'energy': initial_energy,\n",
    "        'odour': [],\n",
    "    }]\n",
    "        \n",
    "    idx = 1\n",
    "    edge_idx = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        graph.append({\n",
    "            'idx': idx,\n",
    "            'type': 'sentence',\n",
    "            'parent': 0,\n",
    "            'edges': [],\n",
    "            'energy': initial_energy,\n",
    "            'odour': [],\n",
    "        })\n",
    "        \n",
    "        graph[idx]['edges'].append({'node': 0, 'idx': edge_idx})        \n",
    "        graph[0]['edges'].append({'node': idx, 'idx': edge_idx})\n",
    "        edge_idx += 1\n",
    "\n",
    "    \n",
    "        sentence_idx = idx\n",
    "        idx += 1\n",
    "        \n",
    "        for word, senses in sentence:\n",
    "            graph.append({\n",
    "                'idx': idx,\n",
    "                'type': 'word',\n",
    "                'parent': sentence_idx,\n",
    "                'edges': [],\n",
    "                'energy': initial_energy,\n",
    "                'odour': [],\n",
    "                \n",
    "                'word': word,\n",
    "            })\n",
    "            \n",
    "            graph[idx]['edges'].append({'node': sentence_idx, 'idx': edge_idx})            \n",
    "            graph[sentence_idx]['edges'].append({'node': idx, 'idx': edge_idx})\n",
    "            edge_idx += 1\n",
    "\n",
    "            word_idx = idx\n",
    "            idx += 1\n",
    "            \n",
    "            for sense in senses:\n",
    "                graph.append({\n",
    "                    'idx': idx,\n",
    "                    'type': 'sense',\n",
    "                    'parent': word_idx,\n",
    "                    'edges': [],\n",
    "                    'energy': initial_energy,\n",
    "                    'odour': [],\n",
    "                    \n",
    "                    'sense': sense\n",
    "                })\n",
    "                \n",
    "                graph[idx]['edges'].append({'node': word_idx, 'pheromone': 0, 'idx': edge_idx})                \n",
    "                graph[word_idx]['edges'].append({'node': idx, 'pheromone': 0, 'idx': edge_idx})\n",
    "                edge_idx += 1\n",
    "                \n",
    "                tokens = sense_to_tokens_set(sense, dictionary, cache, extensions)['simple']\n",
    "                graph[idx]['odour'] = tokens[:odour_length]\n",
    "                \n",
    "                idx += 1\n",
    "        \n",
    "    edges = [{'pheromone': 0} for _ in range(edge_idx)]\n",
    "    \n",
    "    return graph, edges\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a45dab9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== === ===\n",
      "Cycle 1/20\n",
      "Dead ants 0\n",
      "New ants 508\n",
      "Fitness score: 6752\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 2/20\n",
      "Dead ants 0\n",
      "New ants 512\n",
      "Fitness score: 5089\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 3/20\n",
      "Dead ants 0\n",
      "New ants 515\n",
      "Fitness score: 5081\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 4/20\n",
      "Dead ants 0\n",
      "New ants 508\n",
      "Fitness score: 4157\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 5/20\n",
      "Dead ants 0\n",
      "New ants 504\n",
      "Fitness score: 3767\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 6/20\n",
      "Dead ants 508\n",
      "New ants 514\n",
      "Fitness score: 4337\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 7/20\n",
      "Dead ants 512\n",
      "New ants 498\n",
      "Fitness score: 4235\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 8/20\n",
      "Dead ants 515\n",
      "New ants 473\n",
      "Fitness score: 2604\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 9/20\n",
      "Dead ants 508\n",
      "New ants 463\n",
      "Fitness score: 3601\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 10/20\n",
      "Dead ants 504\n",
      "New ants 431\n",
      "Fitness score: 3112\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 11/20\n",
      "Dead ants 514\n",
      "New ants 244\n",
      "Fitness score: 3607\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 12/20\n",
      "Dead ants 498\n",
      "New ants 97\n",
      "Fitness score: 4640\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 13/20\n",
      "Dead ants 473\n",
      "New ants 44\n",
      "Fitness score: 5569\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 14/20\n",
      "Dead ants 463\n",
      "New ants 12\n",
      "Fitness score: 7871\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 15/20\n",
      "Dead ants 431\n",
      "New ants 4\n",
      "Fitness score: 8137\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 16/20\n",
      "Dead ants 244\n",
      "New ants 1\n",
      "Fitness score: 8213\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 17/20\n",
      "Dead ants 97\n",
      "New ants 2\n",
      "Fitness score: 8365\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 18/20\n",
      "Dead ants 44\n",
      "New ants 0\n",
      "Fitness score: 8365\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 19/20\n",
      "Dead ants 12\n",
      "New ants 0\n",
      "Fitness score: 8365\n",
      "=== === ===\n",
      "\n",
      "=== === ===\n",
      "Cycle 20/20\n",
      "Dead ants 4\n",
      "New ants 0\n",
      "Fitness score: 8365\n",
      "=== === ===\n",
      "\n",
      "Solution (score = 8365):\n",
      "# a fangstrom.n.01 = a metric unit of length equal to one ten billionth of a meter (or 0.0001 micron); used to specify wavelengths of electromagnetic radiation\n",
      "\n",
      "# retired fretire.v.01 = go into retirement; stop performing one's work or withdraw from one's position\n",
      "\n",
      "# clerk fclerk.n.01 = an employee who performs clerical work (e.g., keeps records or accounts)\n",
      "\n",
      "# commissariat fcommissariat.n.01 = a stock or supply of foods\n",
      "\n",
      "# department fdepartment.n.01 = a specialized division of a large organization\n",
      "\n",
      "# came fcome.v.01 = move toward, travel toward something or somebody or approach something or somebody\n",
      "\n",
      "# too fexcessively.r.01 = to a degree exceeding normal or proper limits\n",
      "\n",
      "# he fhelium.n.01 = a very light colorless element that is one of the six inert gasses; the most difficult gas to liquefy; occurs in economically extractable amounts in certain natural gases (as those found in Texas and Kansas)\n",
      "\n",
      "# was fwashington.n.02 = a state in northwestern United States on the Pacific\n",
      "\n",
      "# drunk fdrunkard.n.01 = a chronic drinker\n",
      "\n",
      "# had fhave.v.01 = have or possess, either in a concrete or an abstract sense\n",
      "\n",
      "# a fangstrom.n.01 = a metric unit of length equal to one ten billionth of a meter (or 0.0001 micron); used to specify wavelengths of electromagnetic radiation\n",
      "\n",
      "# loud floud.a.01 = characterized by or producing sound of great volume or intensity\n",
      "\n",
      "# most fmost.a.01 = (superlative of `many' used with count nouns and often preceded by `the') quantifier meaning the greatest in number\n",
      "\n",
      "# unseemly findecent.s.01 = not in keeping with accepted standards of what is right or proper in polite society\n",
      "\n",
      "# laugh flaugh.n.01 = the sound of laughing\n",
      "\n",
      "# only flone.s.03 = being the only one; single and isolated from others\n",
      "\n",
      "# fancy fillusion.n.02 = something many people believe that is false\n",
      "\n",
      "# was fwashington.n.02 = a state in northwestern United States on the Pacific\n",
      "\n",
      "# a fangstrom.n.01 = a metric unit of length equal to one ten billionth of a meter (or 0.0001 micron); used to specify wavelengths of electromagnetic radiation\n",
      "\n",
      "# waistcoat fvest.n.01 = a man's sleeveless garment worn underneath a coat\n",
      "\n",
      "# one fone.n.01 = the smallest whole number or a numeral representing this number\n",
      "\n",
      "# visitors fvisitor.n.01 = someone who visits\n",
      "\n",
      "# sat fsaturday.n.01 = the seventh and last day of the week; observed as the Sabbath by Jews and some Christians\n",
      "\n",
      "# straight fheterosexual.n.01 = a heterosexual person; someone having a sexual orientation to persons of the opposite sex\n",
      "\n",
      "# down fdown.n.01 = soft fine feathers\n",
      "\n",
      "# table ftable.n.01 = a set of data arranged in rows and columns\n",
      "\n",
      "# even fevening.n.01 = the latter part of the day (the period of decreasing daylight from late afternoon until nightfall)\n",
      "\n",
      "# greeting fgreeting.n.01 = (usually plural) an acknowledgment or expression of good will (especially on meeting)\n",
      "\n",
      "# finally ffinally.r.01 = after an unspecified period of time or an especially long delay\n",
      "\n",
      "# one fone.n.01 = the smallest whole number or a numeral representing this number\n",
      "\n",
      "# person fperson.n.01 = a human being\n",
      "\n",
      "# having fhave.v.01 = have or possess, either in a concrete or an abstract sense\n",
      "\n",
      "# no fno.n.01 = a negative\n",
      "\n",
      "# suit fsuit.n.01 = a set of garments (usually including a jacket and trousers or skirt) for outerwear all of the same fabric and color\n",
      "\n",
      "# appeared flook.v.02 = give a certain impression or have a certain outward aspect\n",
      "\n",
      "# in finch.n.01 = a unit of length equal to one twelfth of a foot\n",
      "\n",
      "# but fmerely.r.01 = and nothing more\n",
      "\n",
      "# was fwashington.n.02 = a state in northwestern United States on the Pacific\n",
      "\n",
      "# too fexcessively.r.01 = to a degree exceeding normal or proper limits\n",
      "\n",
      "# much fmuch.n.01 = a great amount or extent\n",
      "\n",
      "# efforts fattempt.n.01 = earnest and conscientious activity intended to do or accomplish something\n",
      "\n",
      "# pole fpole.n.01 = a long (usually round) rod of wood or metal or plastic\n",
      "\n",
      "# succeeded fsucceed.v.01 = attain success or reach a desired goal\n",
      "\n",
      "# in finch.n.01 = a unit of length equal to one twelfth of a foot\n",
      "\n",
      "# removing fremove.v.01 = remove something concrete, as by lifting, pushing, or taking off, or remove something abstract\n",
      "\n",
      "# pole fpole.n.01 = a long (usually round) rod of wood or metal or plastic\n",
      "\n",
      "# brought fbring.v.01 = take something or somebody with oneself somewhere\n",
      "\n",
      "# however fhowever.r.01 = despite anything to the contrary (usually following a concession)\n",
      "\n",
      "# two ftwo.n.01 = the cardinal number that is the sum of one and one or a numeral representing this number\n",
      "\n",
      "# other fother.a.01 = not the same one or ones already mentioned or implied; - the White Queen\n",
      "\n",
      "# poles fpole.n.01 = a long (usually round) rod of wood or metal or plastic\n",
      "\n",
      "# who fworld_health_organization.n.01 = a United Nations agency to coordinate international health activities and to help governments improve health services\n",
      "\n",
      "# did fmake.v.01 = engage in\n",
      "\n",
      "# not fnot.r.01 = negation of a word or group of words\n",
      "\n",
      "# live fpopulate.v.01 = inhabit or live in; be an inhabitant of\n",
      "\n",
      "# at fastatine.n.01 = a highly unstable radioactive element (the heaviest of the halogen series); a decay product of uranium and thorium\n",
      "\n",
      "# s fsecond.n.01 = 1/60 of a minute; the basic unit of time adopted under the Systeme International d'Unites\n",
      "\n",
      "# no fno.n.01 = a negative\n",
      "\n",
      "# one fone.n.01 = the smallest whole number or a numeral representing this number\n",
      "\n",
      "# had fhave.v.01 = have or possess, either in a concrete or an abstract sense\n",
      "\n",
      "# seen fsee.v.01 = perceive by sight or have the power to perceive by sight\n",
      "\n",
      "# here fhere.n.01 = the present location; this place\n",
      "\n",
      "# before fearlier.r.01 = earlier in time; previously\n",
      "\n",
      "# all fall.a.01 = quantifier; used with either mass or count nouns to indicate the whole number or amount of or every one of a class\n",
      "\n",
      "# irritated fannoy.v.01 = cause annoyance in; disturb, especially by minor irritations\n",
      "\n",
      "# intensely fintensely.r.01 = in an intense manner\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"A retired clerk of the commissariat department came, too; he was\n",
    "drunk, had a loud and most unseemly laugh and only fancy--was without\n",
    "a waistcoat! One of the visitors sat straight down to the table without\n",
    "even greeting Katerina Ivanovna. Finally one person having no suit\n",
    "appeared in his dressing-gown, but this was too much, and the efforts of\n",
    "Amalia Ivanovna and the Pole succeeded in removing him. The Pole brought\n",
    "with him, however, two other Poles who did not live at Amalia Ivanovna’s\n",
    "and whom no one had seen here before. All this irritated Katerina\n",
    "Ivanovna intensely.\"\"\"\n",
    "\n",
    "ant_colony(text, default_extensions, default_parameters)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
